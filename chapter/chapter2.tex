\setcounter{chapter}{1}
\chapter{Introduction to quantum mechanics}
\Textbf{2.1}

\begin{align*}
	\begin{bmatrix}
		1 \\ 
		-1
	\end{bmatrix}
	+
	\begin{bmatrix}
		1 \\ 
		2
	\end{bmatrix}
	-
	\begin{bmatrix}
		2 \\ 
		1
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\ 
		0
	\end{bmatrix}
\end{align*}

\Textbf{2.2}

\begin{align*}
	A\ket{0} &= A_{11}\ket{0} + A_{21}\ket{1} = \ket{1} \Rightarrow A_{11} = 0,\ A_{21} = 1\\
	A\ket{1} &= A_{12}\ket{0} + A_{22}\ket{1} = \ket{0} \Rightarrow A_{12} = 1,\ A_{22} = 0\\
%
	\therefore A &=
	\begin{bmatrix}
		0 & 1 \\ 
		1 & 0
	\end{bmatrix}
\end{align*}\\

input: $\left\{\ket{0}, \ket{1}\right\}$,
output: $\left\{\ket{1}, \ket{0}\right\}$

\begin{align*}
	A\ket{0}&= A_{11}\ket{1} + A_{21}\ket{0} = \ket{1} \Rightarrow A_{11} = 1,\ A_{21} = 0\\
%
	A\ket{1} &= A_{12}\ket{1} + A_{22}\ket{0} = \ket{0} \Rightarrow A_{12} = 0,\ A_{22} = 1\\
%
	A &=
	\begin{bmatrix}
	1 & 0 \\ 
	0 & 1
	\end{bmatrix}
\end{align*}


\Textbf{2.3}

From eq (2.12)
\begin{align*}
	A \ket{v_i} &= \sum_{j} A_{ji}\ket{w_j}\\
	B \ket{w_j} &= \sum_{k} B_{kj}\ket{x_k}
\end{align*}
%
Thus
\begin{align*}
	BA \ket{v_i} &= B \left( \sum_{j} A_{ji}\ket{w_j} \right)\\
	&= \sum_{j} A_{ji} B\ket{w_j}\\
	&= \sum_{j,k} A_{ji} B_{kj}\ket{x_k}\\
	&= \sum_k \left( \sum_j B_{kj} A_{ji}  \right) \ket{x_k}\\
	&= \sum_k (BA)_{ki} \ket{x_k}\\
	\therefore& (BA)_{ki} = \sum_j B_{kj} A_{ji}
\end{align*}



\Textbf{2.4}
\begin{align*}
	I\ket{v_j} = \sum_i I_{ij} \ket{v_i} = \ket{v_j},\ \forall j.\\
	\Rightarrow I_{ij} = \delta_{ij}
\end{align*}


\Textbf{2.5}

Defined inner product on $\mathcal{C}^n$ is
\begin{align*}
	\left(
		(y_1, \cdots, y_n), (z_1, \cdots, z_n)
	\right)
	= \sum_{i} y_i^* z_i .
\end{align*}
Verify (1) of eq (2.13).

\begin{align*}
	\left(
		(y_1, \cdots, y_n), \sum_i \lambda_i (z_{i1}, \cdots, z_{in})
	\right)
	&= \sum_i y_i^* \left(
										\sum_j \lambda_j z_{ji}
			      				   \right)\\
	&= \sum_{i,j} y_i^* \lambda_j z_{ji}\\
	&= \sum_j \lambda_j \left(\sum_i y_i^* z_{ji}  \right)\\
	&= \sum_j \lambda_j \left(
													(y_1, \cdots, y_n),  (z_{j1}, \cdots, z_{jn})
											  \right)\\
	&= \sum_i \lambda_i \left(
													(y_1, \cdots, y_n),  (z_{i1}, \cdots, z_{in})
												\right).
\end{align*}


Verify (2) of eq (2.13),
\begin{align}
	\left(
		(y_1, \cdots, y_n), (z_1, \cdots, z_n)
	\right)^*
	&= \left(\sum_i y_i^* z_i \right)^*\\
	&= \left(\sum_i y_i  z_i^* \right)\\
	&= \left(\sum_i z_i^* y_i \right)\\
	&= \left(
				(z_1, \cdots, z_n) , (y_1, \cdots, y_n)
			\right)
\end{align}.


Verify (3) of eq (2.13),
\begin{align*}
	\left(
		(y_1, \cdots, y_n), (y_1, \cdots, y_n)
	\right)
	&= \sum_i y_i^* y_i\\
	&= \sum_i |y_i|^2
\end{align*}

Since $|y_i|^2 \geq 0$ for all $i$. Thus
$\sum_i |y_i|^2 =
\left(
	(y_1, \cdots, y_n), (y_1, \cdots, y_n)
\right) \geq 0
$.

From now on,  I will show the following statement,
\begin{align*}
	\left(
		(y_1, \cdots, y_n), (y_1, \cdots, y_n)
	\right) = 0
	\text{ iff }  (y_1, \cdots, y_n) = 0.
\end{align*}
($\Leftarrow$) This is obvious.\\
($\Rightarrow$)
Suppose $\left( (y_1, \cdots, y_n), (y_1, \cdots, y_n) \right) = 0$. Then $\sum_i |y_i|^2 = 0$.

Since $|y_i|^2 \geq 0$ for all $i$, if $\sum_i |y_i|^2 = 0$, then $|y_i|^2 = 0$ for all $i$.
Therefore $|y_i|^2 = 0 \Leftrightarrow y_i = 0$  for all $i$.
Thus,
\begin{align*}
	(y_1, \cdots, y_n) = 0.
\end{align*}

\Textbf{2.6}
\begin{align*}
	\left(\sum_i \lambda_i \ket{w_i},\ \ket{v}\right) &=
	\left(\ket{v},\ \sum_i \lambda_i \ket{w_i}\right)^*\\
	&= \left[\sum_i \lambda_i \left(\ket{v},\ \ket{w_i}  \right) \right]^* (\because \text{linearlity in the 2nd arg.})\\
	&= \sum_i \lambda_i^* \left(\ket{v},\ \ket{w_i} \right)^*\\
	&= \sum_i \lambda_i^* (\ket{w_i},\ \ket{v})
\end{align*}


\Textbf{2.7}

\begin{align*}
	\braket{w | v} &= \begin{bmatrix}
		1 & 1
	\end{bmatrix}
	\begin{bmatrix}
	1 \\ 
	-1
	\end{bmatrix}
	= 1 - 1 = 0\\
%
	\frac{\ket{w}}{\norm{\ket{w}}} &=
	\frac{\ket{w}}{\sqrt{\braket{w|w}}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\ 
	1
	\end{bmatrix}\\
%
	\frac{\ket{v}}{\norm{\ket{v}}} &=
	\frac{\ket{v}}{\sqrt{\braket{v|v}}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\ 
	-1
	\end{bmatrix}
\end{align*}



\Textbf{2.8}

If $k = 1$,
\begin{align*}
	\ket{v_2} &= \frac{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\\
	\braket{v_1 | v_2} &= \bra{v_1} \left(\frac{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\right)\\
		&= \frac{\braket{v_1 | w_2} - \braket{v_1 | w_2}\braket{v_1 | v_1}}{\norm{\ket{w_2} - \braket{v_1 | w_2}\ket{v_1}}}\\
		&= 0.
\end{align*}

Suppose $\left\{v_1, \cdots v_n \right\}$ $(n \leq d-1)$ is a orthonormal basis. Then
\begin{align*}
	\braket{v_j | v_{n+1}} &= \bra{v_j} \left(\frac{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\right)~~(j \leq n)\\
	&= \frac{\braket{v_j | w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\braket{v_j | v_i}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\\
	&= \frac{\braket{v_j | w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\delta_{ij}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\\
	&= \frac{\braket{v_j | w_{n+1}} - \braket{v_j | w_{n+1}}}{\norm{\ket{w_{n+1}} - \sum_{i=1}^{n}\braket{v_i | w_{n+1}}\ket{v_i}}}\\
	&= 0.
\end{align*}
Thus Gram-Schmidt procedure produces an orthonormal basis.


\Textbf{2.9}

\begin{align*}
	\sigma_0 &= I = \ket{0}\bra{0} + \ket{1}\bra{1}\\
	\sigma_1 &= X = \ket{0}\bra{1} + \ket{1}\bra{0}\\
	\sigma_2 &= Y = -i\ket{0}\bra{1} + i\ket{1}\bra{0}\\
	\sigma_3 &= Z = \ket{0}\bra{0} - \ket{1}\bra{1}
\end{align*}


\Textbf{2.10}

\begin{align*}
	\ket{v_j}\bra{v_k} &= I_V \ket{v_j} \bra{v_k} I_V\\
	&= \left(\sum_p \ket{v_p}\bra{v_p} \right) \ket{v_j}\bra{v_k} \left(\sum_q \ket{v_q}\bra{v_q} \right)\\
	&= \sum_{p,q} \ket{v_p} \braket{v_p|v_j}
	\braket{v_k | v_q} \bra{v_q}\\
	&= \sum_{p,q} \delta_{pj} \delta_{kq} \ket{v_p} \bra{v_q}
\end{align*}
Thus
\begin{align*}
	\left( \ket{v_j}\bra{v_k} \right)_{pq} = \delta_{pj} \delta_{kq}
\end{align*}



\Textbf{2.11}

\begin{align*}
	X = \begin{bmatrix}
	0 & 1 \\ 
	1 & 0
	\end{bmatrix},\ \det(X-\lambda I) =
	\det \left(\begin{bmatrix}
	-\lambda & 1 \\ 
	1 & -\lambda
	\end{bmatrix} \right) = 0 \Rightarrow \lambda \pm 1
\end{align*}

If $\lambda = -1$,
\begin{align*}
	\begin{bmatrix}
		1 & 1 \\ 
		1 & 1
	\end{bmatrix}
	\begin{bmatrix}
		c_1 \\ 
		c_2
	\end{bmatrix} =
	\begin{bmatrix}
		0 \\ 
		0
	\end{bmatrix}
\end{align*}
Thus
\begin{align*}
	\ket{\lambda = -1} = \begin{bmatrix}
	c_1 \\ 
	c_2
	\end{bmatrix} = \frac{1}{\sqrt{2}}
	\begin{bmatrix}
	-1 \\ 
	1
	\end{bmatrix}
\end{align*}

If $\lambda = 1$
\begin{align*}
	\ket{\lambda = 1} = \frac{1}{\sqrt{2}}
	\begin{bmatrix}
	1 \\ 
	1
	\end{bmatrix}
\end{align*}

\begin{align*}
	X = \begin{bmatrix}
	-1 & 0 \\ 
	0 & 1
	\end{bmatrix}
	\text{ w.r.t. } \left\{ \ket{\lambda = -1},\ \ket{\lambda = 1}\right\}
\end{align*}



\Textbf{2.12}

\begin{align*}
	\det \left(\begin{bmatrix}
	1 & 0 \\ 
	1 & 1
	\end{bmatrix} - \lambda I \right) = (1 - \lambda)^2 = 0 \Rightarrow \lambda = 1
\end{align*}
Therefore the eigenvector associated with eigenvalue $\lambda = 1$ is
\begin{align*}
	\ket{\lambda = 1} = \begin{bmatrix}
	0 \\ 
	1
	\end{bmatrix}
\end{align*}

Because $\ket{\lambda = 1}\bra{\lambda = 1} = \begin{bmatrix}
0 & 0 \\ 
0 & 1
\end{bmatrix}$,
\begin{align*}
	\begin{bmatrix}
	1 & 0 \\ 
	1 & 1
	\end{bmatrix} \neq c\ket{\lambda = 1}\bra{\lambda = 1} = \begin{bmatrix}
	0 & 0 \\ 
	0 & c
	\end{bmatrix}
\end{align*}



\Textbf{2.13}
Suppose $\ket{\psi},\ \ket{\phi}$ are arbitrary vectors in $V$.
\begin{align*}
	\left(\ket{\psi},\ (\ket{w}\bra{v}) \ket{\phi}\right)^* &=
	\left((\ket{w}\bra{v})^\dagger \ket{\psi},\  \ket{\phi}\right)^*\\
	&= \left(\ket{\phi},\ (\ket{w}\bra{v})^\dagger \ket{\psi} \right)\\
	&= \bra{\phi} (\ket{w}\bra{v})^\dagger \ket{\psi}.
\end{align*}

On the other hand,
\begin{align*}
	\left(\ket{\psi},\ (\ket{w}\bra{v}) \ket{\phi}\right)^*
	&= (\braket{\psi | w} \braket{v | \phi})^*\\
	&= \braket{\phi | v} \braket{w | \psi}.
\end{align*}

Thus
\begin{align*}
	\bra{\phi} (\ket{w}\bra{v})^\dagger \ket{\psi} = \braket{\phi | v} \braket{w | \psi} \text{ for arbitrary vectors } \ket{\psi},\ \ket{\phi}\\
	\therefore (\ket{w}\bra{v})^\dagger = \ket{v}\bra{w}
\end{align*}


\Textbf{2.14}
\begin{align*}
	( (a_i A_i)^\dagger \ket{\phi},\ \ket{\psi} )
	&= (\ket{\phi},\ a_i A_i \ket{\psi})\\
	&= a_i (\ket{\phi},\ A_i \ket{\psi})\\
	&= a_i (A_i^\dagger \ket{\phi},\ \ket{\psi})\\
	&= (a_i^* A_i^\dagger \ket{\phi},\ \ket{\psi})\\
%
	\therefore (a_i A_i)^\dagger = a_i^* A_i^\dagger
\end{align*}




\Textbf{2.15}
\begin{align*}
	((A^\dagger)^\dagger\ket{\psi},\ \ket{\phi} )
	&= (\ket{\psi},\ A^\dagger \ket{\phi})\\
	&= (A^\dagger \ket{\phi},\ \ket{\psi})^*\\
	&= (\ket{\phi},\ A\ket{\psi})^*\\
	&= (A\ket{\psi},\ \ket{\phi})\\
	\therefore (A^\dagger)^\dagger = A
\end{align*}


\Textbf{2.16}
\begin{align*}
	P &= \sum_i \ket{i}\bra{i}.\\
	P^2 &= \left(\sum_i \ket{i}\bra{i}\right) \left(\sum_j \ket{j}\bra{j}\right)\\
	&= \sum_{i,j} \ket{i}\braket{i | j}\bra{j}\\
	&= \sum_i \ket{i}\bra{j} \delta_{ij}\\
	&= \sum_i \ket{i}\bra{i}\\
	&= P
\end{align*}




\Textbf{2.18}

Suppose $\ket{v}$ is a eigenvector with corresponding eigenvalue $\lambda$.
\begin{align*}
	U \ket{v} &= \lambda \ket{v}.\\
	1 &= \braket{v | v}\\
	&= \bra{v} I \ket{v}\\
	&= \bra{v} U^\dagger U \ket{v}\\
	&= \lambda \lambda^* \braket{v | v}\\
	&= \norm{\lambda}^2\\
	\therefore \lambda &= e^{i \theta}
\end{align*}




\Textbf{2.19}
\begin{align*}
	X^2 = \begin{bmatrix}
		0 & 1 \\ 
		1 & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 1 \\ 
		1 & 0
	\end{bmatrix}
	= \begin{bmatrix}
		1 & 0 \\ 
		0 & 1
	\end{bmatrix} = I
\end{align*}



\Textbf{2.20}
\begin{align*}
	U &\equiv \sum_i \ket{w_i}\bra{v_i}\\
	A_{ij}^{'} &= \braket{v_i | A | v_j}\\
	&= \braket{v_i | UU^\dagger A UU^\dagger | v_j}\\
	&= \sum_{p,q,r,s} \braket{v_i | w_p} \braket{v_p | v_q} \braket{w_q | A | w_r} \braket{v_r | v_s} \braket{w_s | v_j}\\
	&= \sum_{p,q,r,s} \braket{v_i | w_p} \delta_{pq} A_{qr}^{''} \delta_{rs}  \braket{w_s | v_j}\\
	&= \sum_{p,r}  \braket{v_i | w_p}  \braket{w_r | v_j} A_{pr}^{''}
\end{align*}


\Textbf{2.21}
Suppose $M$ be Hermitian. Then $M = M^\dagger$.
\begin{align*}
	M &= IMI\\
		&= (P+Q) M (P+Q)\\
		&= PMP + QMP + PMQ + QMQ\\
\end{align*}
Now $PMP = \lambda P$, $QMP = 0$, $PMQ = PM^\dagger Q = (QMP)^* = 0$.
Thus $M = PMP + QMQ$.
Next prove $QMQ$ is normal.
\begin{align*}
	QMQ (QMQ)^\dagger &= QMQ QM^\dagger Q\\
		&= QM^\dagger Q QMQ ~~~ (M = M^\dagger)\\
		&= (QM^\dagger Q) QMQ
\end{align*}
Therefore $QMQ$ is normal.
By induction, $QMQ$ is diagonal ... (following is same as Box 2.2)

\Textbf{2.22}
Suppose $A$ is a Hermitian operator and $\ket{v_i}$ are eigenvectors of $A$ with eigenvalues $\lambda_i$.
Then
\begin{align*}
	\braket{v_i | A | v_j} = \lambda_j \braket{v_i | v_j}.
\end{align*}

On the other hand,
\begin{align*}
	\braket{v_i | A | v_j} = \braket{v_i | A^\dagger | v_j}
	= \braket{v_j | A | v_i}^*
	= \lambda_i^* \braket{v_j | v_i}^*
	=  \lambda_i^* \braket{v_i | v_j}
	=  \lambda_i \braket{v_i | v_j}
\end{align*}

Thus
\begin{align*}
	(\lambda_i - \lambda_j) \braket{v_i | v_j}  = 0.
\end{align*}
If $\lambda_i \neq \lambda_j$, then $\braket{v_i | v_j}  = 0$.


\Textbf{2.23}
Suppose $P$ is projector and $\ket{\lambda}$  are eigenvectors of $P$ with eigenvalues $\lambda$.
Then $P^2 = P$.
\begin{align*}
	P \ket{\lambda} = \lambda \ket{\lambda} \text{ and }	P \ket{\lambda} = P^2 \ket{\lambda} = \lambda  P \ket{\lambda} = \lambda^2 \ket{\lambda}.
\end{align*}

Therefore
\begin{align*}
	\lambda = \lambda^2\\
	\lambda (\lambda - 1) = 0\\
	\lambda = 0 \text{ or } 1.
\end{align*}


\Textbf{2.24}
Def of positive $\braket{v | A | v} \geq 0$ for all $\ket{v}$.

Suppose $A$ is a positive operator. $A$ can be decomposed as follows.
\begin{align*}
	A &= \frac{A + A^\dagger}{2} + i \frac{A - A^\dagger}{2i}\\
		&= B + i C  ~~~\text{where } B =\frac{A + A^\dagger}{2}, ~~  C = \frac{A - A^\dagger}{2i}.
\end{align*}
Now operators $B$ and $C$ are Hermitian.

\begin{align*}
	\braket{v | A | v}  &= \braket{v | B + iC | v}\\
		&= \braket{v | B | v}  + i \braket{v | C | v}\\
		&= \alpha + i \beta ~~ \text{where } \alpha = \braket{v | B | v}, ~\beta = \braket{v | C | v}.
\end{align*}

Since $B$ and $C$ are Hermitian, $\alpha,~ \beta \in \mathds{R}$.
From def of positive operator, $\beta$ should be vanished.
Hence $\beta = \braket{v | C | v}$ for all $\ket{v}$, i.e. $C = 0$.

Therefore $A = B$. Since $B$ is Hermitian, positive operator $A$ is also Hermitian.


\Textbf{2.25}
\begin{align*}
	\braket{\psi | A^\dagger A | \psi} = \norm{A \ket{\psi}}^2 \geq 0 \text{ for all } \ket{\psi}.
\end{align*}
Thus $A^\dagger A$ is positive.


\Textbf{2.26}
\begin{align*}
	\ket{\psi}^{\otimes 2} &= \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}) \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}\\
		&= \frac{1}{2} (\ket{00}  + \ket{01} + \ket{10} + \ket{11}  )\\
		&= \frac{1}{2} \begin{bmatrix}
			1 \\ 
			1 \\ 
			1 \\ 
			1
		\end{bmatrix}
\end{align*}

\begin{align*}
	\ket{\psi}^{\otimes 3} &= \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}) \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}  \otimes \frac{1}{\sqrt{2}} (\ket{0} + \ket{1}\\
		&= \frac{1}{2\sqrt{2}} (\ket{000}  + \ket{001} + \ket{010} + \ket{011} +  \ket{100}  + \ket{101} + \ket{110} + \ket{111})\\
		&= \frac{1}{2\sqrt{2}} \begin{bmatrix}
			1 \\ 
			1 \\ 
			1 \\ 
			1 \\
			1 \\
			1 \\
			1 \\
			1
		\end{bmatrix}
\end{align*}


\Textbf{2.27}
\begin{align*}
	X \otimes Z &= \begin{bmatrix}
		0 & 1 \\ 
		1 & 0
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		1 & 0 \\ 
		0 & -1
	\end{bmatrix} \\
	&= \begin{bmatrix}
		0 & 0 & 1 & 0 \\ 
		0 & 0 & 0 & -1 \\ 
		1 & 0 & 0 & 0 \\ 
		0 & -1 & 0 & 0
	\end{bmatrix}
\end{align*}

\begin{align*}
	I \otimes X &= \begin{bmatrix}
		1 & 0 \\ 
		0 & 1
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		0 & 1 \\ 
		1 & 0
	\end{bmatrix}\\
	&=
	\begin{bmatrix}
	0 & 1 & 0 & 0 \\ 
	1 & 0 & 0 & 0 \\ 
	0 & 0 & 0 & 1 \\ 
	0 & 0 & 1 & 0
	\end{bmatrix}
\end{align*}

\begin{align*}
	X \otimes I &= \begin{bmatrix}
		0 & 1 \\ 
		1 & 0
	\end{bmatrix}
	\otimes
	\begin{bmatrix}
		1 & 0 \\ 
		0 & 1
	\end{bmatrix}\\
	&= \begin{bmatrix}
	0 & 0 & 1 & 0 \\ 
	0 & 0 & 0 & 1 \\ 
	1 & 0 & 0 & 0 \\ 
	0 & 1 & 0 & 0
	\end{bmatrix}
\end{align*}

In general, tensor product is not commutable.



\Textbf{2.28}
\begin{align*}
	(A \otimes B)^*
	&=
	\begin{bmatrix}
		A_{11} B & \cdots & A_{1n} B \\ 
		\vdots & \ddots  & \vdots \\ 
		A_{m1}B & \cdots & A_{mn} B
	\end{bmatrix}^* \\
	&=
	\begin{bmatrix}
		A_{11}^* B^* & \cdots & A_{1n}^* B^* \\ 
		\vdots & \ddots  & \vdots \\ 
		A_{m1}^* B^* & \cdots & A_{mn}^* B^*
	\end{bmatrix} \\
	&= A^* \otimes B^*.
\end{align*}


\begin{align*}
	(A\otimes B)^T &=
	\begin{bmatrix}
		A_{11} B & \cdots & A_{1n} B \\ 
		\vdots & \ddots  & \vdots \\ 
		A_{m1}B & \cdots & A_{mn} B
	\end{bmatrix}^T \\
	&=
	\begin{bmatrix}
		A_{11} B^T & \cdots & A_{m1} B^T \\ 
		\vdots & \ddots  & \vdots \\ 
		A_{1n} B^T & \cdots & A_{mn} B^T
	\end{bmatrix} \\
	&=
	\begin{bmatrix}
		A_{11} B^T & \cdots & A_{1m}^T B^T \\ 
		\vdots & \ddots  & \vdots \\ 
		A_{n1}^T B^T & \cdots & A_{nm}^T B^T
	\end{bmatrix} \\
	&= A^T \otimes B^T.
\end{align*}


\begin{align*}
	(A\otimes B)^\dagger&=((A \otimes B)^*)^T	\\
		&= (A^* \otimes B^*)^T\\
		&= (A^*)^T \otimes (B^*)^T\\
		&= A^\dagger \otimes B^\dagger.
\end{align*}

\Textbf{2.29}
Suppose $U_1$ and $U_2$ are unitary operators. Then
\begin{align*}
	(U_1 \otimes U_2) (U_1 \otimes U_2)^\dagger &=U_1 U_1^\dagger \otimes U_2 U_2^\dagger\\
		&= I \otimes I.
\end{align*}

Similarly,
\begin{align*}
	(U_1 \otimes U_2)^\dagger (U_1 \otimes U_2)  = I \otimes I.
\end{align*}

\Textbf{2.30}
Suppose $A$ and $B$ are Hermitian operators. Then
\begin{align}
(A \otimes B)^\dagger = A^\dagger \otimes B^\dagger = A \otimes B.
\end{align}
Thus $A \otimes B$ is Hermitian.



\Textbf{2.31}
Suppose $A$ and $B$ are positive operators. Then
\begin{align*}
	\bra{\psi} \otimes \bra{\phi} (A \otimes B) \ket{\psi} \otimes \ket{\phi} &= \braket{\psi |A| \psi} \braket{\phi | B | \phi}.
\end{align*}

Since $A$ and $B$ are positive operators,
$\braket{\psi |A| \psi} \geq 0$ and $\braket{\phi | B | \phi} \geq 0$ for all $\ket{\psi}, \ket{\phi} $.
Then $\braket{\psi |A| \psi} \braket{\phi | B | \phi} \geq 0$.
Thus $A \otimes B$ is positive if $A$ and $B$ are positive.


\Textbf{2.32}
Suppose $P_1$ and $P_2$  are projectors. Then
\begin{align*}
	(P_1 \otimes P_2) ^2 &= P_1^2 \otimes P_2^2\\
		&= P_1 \otimes P_2.
\end{align*}
Thus $ P_1 \otimes P_2.$ is also projector.


\Textbf{2.33}
\begin{align}
	H  = \frac{1}{\sqrt{2}} \begin{bmatrix}
		1 & 1 \\ 
		1 & -1
	\end{bmatrix}
\end{align}

\begin{align*}
	H^{\otimes 2}
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}
	\otimes
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{2} \begin{bmatrix}
		1 & 1 & 1 & 1 \\ 
		1 & -1 & 1 & -1 \\ 
		1 & 1 & -1 & -1 \\ 
		1 & -1 & -1 & 1
	\end{bmatrix}
\end{align*}

\Textbf{2.34}
Suppose $A = \begin{bmatrix}
4 & 3 \\ 
3 & 4
\end{bmatrix} $.

\begin{align*}
	\det (A - \lambda I ) &= (4-\lambda)^2 - 3^2\\
		&= \lambda^2 -8\lambda + 7\\
		&= (\lambda - 1)(\lambda - 7)
\end{align*}

Eigenvalues of $A$ are $\lambda = 1, ~ 7$.
Corresponding eigenvectors are
$
	\ket{\lambda = 1} = \frac{1}{\sqrt{2}} \begin{bmatrix}
	1 \\ 
	-1
	\end{bmatrix}
$,
$
	\ket{\lambda = 7} = \frac{1}{\sqrt{2}} \begin{bmatrix}
1 \\ 
1
\end{bmatrix}
$.

\vspace{5mm}
Thus
\begin{align*}
	A = \kb{\lambda = 1} + 7 \kb{\lambda = 7}.
\end{align*}

\begin{align*}
	\sqrt{A} &= \kb{\lambda = 1} + \sqrt{7} \kb{\lambda = 7}\\
		&= \frac{1}{2} \begin{bmatrix}
		1 & -1 \\ 
		-1 & 1
		\end{bmatrix}
		+
		\frac{\sqrt{7}}{2} \begin{bmatrix}
		1 & 1 \\ 
		1 & 1
		\end{bmatrix}\\
		&=
		\frac{1}{2}
		 \begin{bmatrix}
			1+\sqrt{7} & -1+\sqrt{7} \\ 
			-1 + \sqrt{7} & 1+\sqrt{7}
		\end{bmatrix}
\end{align*}

 \begin{align*}
 	\log (A) &=  \log (1) \kb{\lambda = 1} + \log (7) \kb{\lambda = 7}\\
 		&= \frac{\log (7)}{2} \begin{bmatrix}
	 		1 & 1 \\ 
	 		1 & 1
 		\end{bmatrix}
 \end{align*}



\Textbf{2.35}
\begin{align*}
	\vec{v} \cdot \vec{\sigma} &= \sum_{i=1}^3 v_i \sigma_i\\
		&= v_1 \begin{bmatrix}
		0 & 1 \\ 
		1 & 0
		\end{bmatrix}
		+ v_2 \begin{bmatrix}
		0 & -i \\ 
		i & 0
		\end{bmatrix}
		+ v_3 \begin{bmatrix}
		1 & 0 \\ 
		0 & -1
		\end{bmatrix} \\
		&= \begin{bmatrix}
		v_3 & v_1 - i v_2 \\ 
		v_1 + iv_2 & -v_3
		\end{bmatrix}
\end{align*}

\begin{align*}
	\det (\vec{v} \cdot \vec{\sigma}  - \lambda I) &= (v_3 - \lambda) (-v_3 - \lambda) - (v_1 - iv_2) (v_1 + iv_2)\\
			&= \lambda^2 - (v_1^2 + v_2^2  + v_3^2)\\
			&= \lambda^2 - 1 ~~~ (\because |\vec{v}| = 1)
\end{align*}
Eigenvalues are $\lambda = \pm 1$.
Let $\ket{\lambda_{ \pm 1 } }$ be eigenvectors with eigenvalues $\pm  1$.

Since $\vec{v} \cdot \vec{\sigma}$ is Hermitian,  $\vec{v} \cdot \vec{\sigma}$ is diagonalizable.
Then
\begin{align*}
	\vec{v} \cdot \vec{\sigma} = \kb{\lambda_1} - \kb{\lambda_{-1}}
\end{align*}

Thus
\begin{align*}
	\exp \left(i \theta \vec{v} \cdot \vec{\sigma} \right) &=
	e^{i \theta} \kb{\lambda_1}  + e^{-i \theta} \kb{\lambda_{-1}}\\
	&= (\cos \theta + i \sin \theta) \kb{\lambda_1} + (\cos \theta - i \sin \theta) \kb{\lambda_{-1}}\\
	&= \cos \theta (\kb{\lambda_1} + \kb{\lambda_{-1}}) + i \sin \theta (\kb{\lambda_1} - \kb{\lambda_{-1}}) \\
	&= \cos( \theta) I + i \sin (\theta) \vec{v} \cdot \vec{\sigma}.
\end{align*}

$\because$ Since $\vec{v} \cdot \vec{\sigma}$ is Hermitian, $\ket{\lambda_1}$ and $\ket{\lambda_{-1}}$ are orthogonal.
Thus
\begin{align*}
	\kb{\lambda_1} + \kb{\lambda_{-1}} = I.
\end{align*}


\Textbf{2.36}
\begin{align*}
	\Tr (\sigma_1) &= \Tr \left(
		\begin{bmatrix}
		0 & 1 \\ 
		1 & 0
		\end{bmatrix}
	\right) = 0\\
%
	\Tr (\sigma_2) &= \Tr \left(
		\begin{bmatrix}
		0 & -i \\ 
		i & 0
		\end{bmatrix}
	\right) = 0\\
%
	\Tr (\sigma_3) &= \Tr \left(
	\begin{bmatrix}
		1 & 0 \\ 
		0 & -1
	\end{bmatrix}
	\right) = 1 -1 = 0\\
\end{align*}



\Textbf{2.37}
\begin{align*}
	\Tr (AB) &= \sum_i \braket{i | AB | i}\\
		&=\sum_i \braket{i | A I B | i}\\
		&= \sum_{i,j} \braket{i | A | j}\braket{j | B | i}\\
		&= \sum_{i,j} \braket{j | B | i} \braket{i | A | j}\\
		&= \sum_j \braket{j | BA | j}\\
		&= \Tr (BA)
\end{align*}

\Textbf{2.38}
\begin{align*}
	\Tr (A + B) &= \sum_i \braket{i | A+B | i}\\
		&= \sum_i (\braket{i|A|i}  + \braket{i | B | i}  )\\
		&= \sum_i \braket{i|A|i} + \sum_i \braket{i|B|i}\\
		&= \Tr (A) + \Tr (B).
\end{align*}

\begin{align*}
	\Tr (z A) &=  \sum_i \braket{i | z A | i}\\	
		&= \sum_i z \braket{i | A | i}\\
		&= z \sum_i \braket{i | A | i}\\
		&= z \Tr (A).
\end{align*}



\Textbf{2.39}
(1)

$(A, B) \equiv \Tr (A^\dagger B)$.

\vspace{5mm}
(i)
\begin{align*}
	\left(A, \sum_i \lambda_i B_i \right) &= \Tr \left[ A^\dagger \left(\sum_i \lambda_i B_i  \right) \right]\\
		&= \Tr (A^\dagger \lambda_1 B_1) + \cdots +  \Tr (A^\dagger \lambda_n B_n) ~~~ (\because \text{Execise 2.38}) \\
		&= \lambda_1 \Tr (A^\dagger B_1)  + \cdots  + \lambda_n \Tr (A^\dagger B_n) \\
		&= \sum_i \lambda_i \Tr (A^\dagger B_i)
\end{align*}


(ii)
\begin{align*}
	(A, B)^* &= \left( \Tr (A^\dagger B) \right)^*\\
		&= \left(\sum_{i,j} \braket{ i | A^\dagger | j} \braket{j | B | i}  \right)^*\\
		&= \sum_{i,j} \braket{ i | A^\dagger | j}^* \braket{j | B | i}^*\\
		&= \sum_{i,j}  \braket{j | B | i}^* \braket{ i | A^\dagger | j}^*\\
		&=  \sum_{i,j}  \braket{i | B^\dagger | j} \braket{ j | A | i}\\
		&= \sum_i \braket{i | B^\dagger A | i} \\
		&= \Tr (B^\dagger A)\\
		&= (B, A).
\end{align*}


(iii)
\begin{align*}
	(A, A) &= \Tr (A^\dagger A)\\
		&= \sum_i \braket{ i | A^\dagger A | i }
\end{align*}
Since $A^\dagger A$ is positive, $\braket{ i | A^\dagger A | i } \geq 0$ for all $\ket{ i }$.


Let $a_i$ be i-th column of $A$.
If $\braket{ i | A^\dagger A | i } = 0$, then
\begin{align*}
	\braket{ i | A^\dagger A | i } = a^\dagger_i a_i = \norm{a_i}^2 = 0 \text{ iff }a_i = \mathbf{0}.
\end{align*}

Therefore $(A, A) = 0$ iff $A = \mathbf{0}$.

\vspace{5mm}
(2)

(3)


%\begin{bmatrix}
%	0 & 1 \\ 
%	1 & 0
%\end{bmatrix}
%
%\begin{bmatrix}
%	0 & -i \\ 
%	i & 0
%\end{bmatrix}
%
%\begin{bmatrix}
%	1 & 0 \\ 
%	0 & -1
%\end{bmatrix}

\Textbf{2.40}
\begin{align*}
	\left[X, Y \right] &=XY - YX\\	
		&= \begin{bmatrix}
		0 & 1 \\ 
		1 & 0
		\end{bmatrix}
		\begin{bmatrix}
		0 & -i \\ 
		i & 0
		\end{bmatrix}
		-
		\begin{bmatrix}
		0 & -i \\ 
		i & 0
		\end{bmatrix}
		\begin{bmatrix}
		0 & 1 \\ 
		1 & 0
		\end{bmatrix} \\
%
		&=
%
		\begin{bmatrix}
			i & 0 \\ 
			0 & -i
		\end{bmatrix}
		-
		\begin{bmatrix}
			-i & 0 \\ 
			0 & i
		\end{bmatrix}\\
%
		&=
%
		\begin{bmatrix}
			2i & 0 \\ 
			0 & -2i
		\end{bmatrix} \\
%
		&=	2i Z
\end{align*}



\begin{align*}
	\left[Y, Z \right] &= \begin{bmatrix}
		0 & -i \\ 
		i & 0
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 \\ 
		0 & -1
	\end{bmatrix}
	-
	\begin{bmatrix}
		1 & 0 \\ 
		0 & -1
	\end{bmatrix}
	\begin{bmatrix}
		0 & -i \\ 
		i & 0
	\end{bmatrix}\\
	&=
	\begin{bmatrix}
		0 & 2i \\ 
		2i & 0
	\end{bmatrix}\\
	&= 2iX
\end{align*}



\begin{align*}
	\left[Z, X\right] &= \begin{bmatrix}
	1 & 0 \\ 
	0 & -1
	\end{bmatrix}
	\begin{bmatrix}
	0 & 1 \\ 
	1 & 0
	\end{bmatrix}
	-
	\begin{bmatrix}
	0 & 1 \\ 
	1 & 0
	\end{bmatrix}
	\begin{bmatrix}
	1 & 0 \\ 
	0 & -1
	\end{bmatrix}\\
	&=
	2i \begin{bmatrix}
	0 & -i \\ 
	i & 0
	\end{bmatrix}\\
	&= 2iY
\end{align*}



\Textbf{2.41}
\begin{align*}
\left\{\sigma_1, \sigma_2 \right\} &=\sigma_1 \sigma_2 + \sigma_2 \sigma_1\\	
&= \begin{bmatrix}
0 & 1 \\ 
1 & 0
\end{bmatrix}
\begin{bmatrix}
0 & -i \\ 
i & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & -i \\ 
i & 0
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\ 
1 & 0
\end{bmatrix} \\
%
&=
%
\begin{bmatrix}
i & 0 \\ 
0 & -i
\end{bmatrix}
+
\begin{bmatrix}
-i & 0 \\ 
0 & i
\end{bmatrix}\\
%
&= 0
\end{align*}



\begin{align*}
\left\{\sigma_2, \sigma_3 \right\} &= \begin{bmatrix}
0 & -i \\ 
i & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\ 
0 & -1
\end{bmatrix}
+
\begin{bmatrix}
1 & 0 \\ 
0 & -1
\end{bmatrix}
\begin{bmatrix}
0 & -i \\ 
i & 0
\end{bmatrix}\\
&=0
\end{align*}



\begin{align*}
\left\{\sigma_3, \sigma_1 \right\} &= \begin{bmatrix}
1 & 0 \\ 
0 & -1
\end{bmatrix}
\begin{bmatrix}
0 & 1 \\ 
1 & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & 1 \\ 
1 & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\ 
0 & -1
\end{bmatrix}\\
&=0
\end{align*}

\begin{align*}
	\sigma_0^2 &= I^2 = I\\
%
	\sigma_1^2 &= \begin{bmatrix}
	0 & 1 \\ 
	1 & 0
	\end{bmatrix} ^2 = I\\
%
	\sigma_2^2 &= \begin{bmatrix}
	0 & -i \\ 
	i & 0
	\end{bmatrix} ^2 = I\\
%
	\sigma_3^2 &= \begin{bmatrix}
	1 & 0 \\ 
	0 & -1
	\end{bmatrix} ^2 = I
\end{align*}



\Textbf{2.42}
\begin{align*}
	\frac{\left[A, B \right] + \left\{A, B\right\}}{2} = \frac{AB - BA + AB + BA}{2} = AB
\end{align*}


\Textbf{2.43}
From eq (2.75) and eq (2.76), $\left\{\sigma_j,  \sigma_k \right\} = 2 \delta_{jk} I$.
From eq (2.77),
\begin{align*}
	\sigma_j \sigma_k &= \frac{\left[\sigma_j, \sigma_k  \right] + \left\{\sigma_j, \sigma_k \right\}}{2}\\
		&= \frac{2i \sum_{l=1}^{3} \epsilon_{jkl}\sigma_l +  2 \delta_{jk} I}{2}\\
		&= \delta_{jk} I + i \sum_{l=1}^{3} \epsilon_{jkl}\sigma_l
\end{align*}


\Textbf{2.44}
By assumption, $\left[A, B\right] = 0$ and $\left\{A, B\right\} = 0$, then $AB = 0$.
Since $A$ is invertible, multiply by $A^{-1}$ from left, then
\begin{align*}
	A^{-1} AB = 0\\
	IB = 0\\
	B=0.
\end{align*}


\Textbf{2.45}
\begin{align*}
	\left[A, B\right]^\dagger &= (AB -BA)^\dagger\\
		&= B^\dagger A^\dagger - A^\dagger B^\dagger\\
		&= \left[B^\dagger, A^\dagger \right]
\end{align*}


\Textbf{2.46}
\begin{align*}
	\left[A, B\right] &= AB - BA\\
		&= - (BA - AB)\\
		&= -\left[B, A\right]
\end{align*}

\Textbf{2.47}
\begin{align*}
	\left(i \left[A, B\right] \right)^\dagger &= -i \left[A, B\right]^\dagger\\
		&= -i \left[B^\dagger, A^\dagger \right]\\
		&= -i \left[B, A \right]\\
		&= i \left[A, B\right]
\end{align*}

\Textbf{2.48}

(Positive )

 Since $P$ is positive, it is diagonalizable. Then $P = \sum_i \lambda_i \kb{i}$, $(\lambda_i \geq 0)$.
\begin{align*}
	J = \sqrt{P^\dagger P} = \sqrt{P P} = \sqrt{P^2} = \sum_i \sqrt{\lambda_i^2} \kb{i} = \sum_i \lambda_i \kb{i} = P.
\end{align*}
 Therefore polar decomposition of $P$ is $P = UP$ for all $P$.
 Thus $U = I$, then $P = P$.


\vspace{5mm}
(Unitary)

Suppose unitary $U$ is decomposed by $U = WJ$ where $W$ is unitary and $J$ is positive, $J = \sqrt{U^\dagger U}$.
\begin{align*}
	J = \sqrt{U^\dagger U} = \sqrt{I} = I
\end{align*}
Since unitary operators are invertible, $W = UJ^{-1} = UI^{-1} = UI = U$.
Thus polar decomposition of $U$ is $U = U$.


\vspace{5mm}
(Hermitian)

Suppose $H = UJ$.
\begin{align*}
	J = \sqrt{H^\dagger H} = \sqrt{HH} = \sqrt{H^2}.
\end{align*}
Thus $H = U\sqrt{H^2}$.

\begin{screen}
	In general, $H \neq \sqrt{H^2}$.

	From spectral decomposition, $H = \sum_i \lambda_i \kb{i}$, $\lambda_i \in \mathds{R}$.
	\begin{align*}
		 \sqrt{H^2} = \sqrt{ \sum_i \lambda_i^2 \kb{i} }
		 =
 		\sum_i
 			\sqrt{
 				\lambda_i^2
			} \kb{i}
		= \sum_i | \lambda_i | \kb{i} \neq H
	\end{align*}
\end{screen}


\Textbf{2.49}
Normal matrix is diagonalizable, $A = \sum_i \lambda_i \kb{i}$.
\begin{align*}
	J &= \sqrt{A^\dagger A} = \sum_i | \lambda_i | \kb{i}.\\
	U &= \sum_i \kbt{e_i}{i}\\
	A &= UJ = \sum_i |\lambda_i| \kbt{e_i}{i}.
\end{align*}



\Textbf{2.50}

Define
$A = \begin{bmatrix}
1 & 0 \\ 
1 & 1
\end{bmatrix}$.
%
$A^\dagger A = \begin{bmatrix}
2 & 1 \\ 
1 & 1
\end{bmatrix}$.

 Characteristic equation of $A^\dagger A$ is $\det(A^\dagger A - \lambda I) = \lambda^2 - 3 \lambda + 1 = 0$.
 Eigenvalues of $A^\dagger A$ are $\lambda_\pm = \frac{3 \pm \sqrt{5}}{2}$
 and associated eigenvectors are $\ket{\lambda_\pm} = \frac{1}{\sqrt{10 \mp 2 \sqrt{5}}} \begin{bmatrix}
 2 \\ 
 -1 \pm \sqrt{5}
 \end{bmatrix} $.

\begin{align*}
	A^\dagger A = \lambda_+ \kb{\lambda_+} + \lambda_- \kb{\lambda_-}.
\end{align*}

 \begin{align*}
 	J = \sqrt{A^\dagger A} &= \sqrt{\lambda_+} \kb{\lambda_+} + \sqrt{\lambda_-} \kb{\lambda_-}\\
 		&= \sqrt{\frac{3 + \sqrt{5}}{2}} \cdot \frac{5 - \sqrt{5}}{40} \begin{bmatrix}
 		4 & 2\sqrt{5} -2 \\ 
 		2 \sqrt{5} - 2 & 6 -2\sqrt{5}
 		\end{bmatrix}
 		+
 		\sqrt{\frac{3 - \sqrt{5}}{2}} \cdot \frac{5 + \sqrt{5}}{40} \begin{bmatrix}
 		4 & -2\sqrt{5} -2 \\ 
 		-2 \sqrt{5} - 2 & 6 +2\sqrt{5}
 		\end{bmatrix}
 \end{align*}


\begin{align*}
	J^{-1} = \frac{1}{ \sqrt{\lambda_+} } \kb{\lambda_+} + \frac{1}{ \sqrt{\lambda_-} } \kb{\lambda_-}.
\end{align*}


\begin{align*}
	U = AJ^{-1}
\end{align*}

I'm tired.

\Textbf{2.51}

\begin{align*}
	H^\dagger H = \left(\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}\right)^\dagger
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}
	=
	\frac{1}{2} \begin{bmatrix}
	2 & 0 \\ 
	0 & 2
	\end{bmatrix}
	=
	I.
\end{align*}


\Textbf{2.52}

\begin{align*}
	H^\dagger = \left(\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}\right)^\dagger
	=
	\frac{1}{\sqrt{2}} \begin{bmatrix}
	1 & 1 \\ 
	1 & -1
	\end{bmatrix}
	=
	H.
\end{align*}

Thus
\begin{align*}
	H^2 = I.
\end{align*}



\Textbf{2.53}

\begin{align*}
	\det \left(H - \lambda I\right) &= \left(\frac{1}{\sqrt{2}} - \lambda \right) \left(- \frac{1}{\sqrt{2}} - \lambda \right) - \frac{1}{2}\\
		&= \lambda^2 - \frac{1}{2} - \frac{1}{2}\\
		&= \lambda^2 - 1
\end{align*}

Eigenvalues are $\lambda_\pm = \pm 1$ and associated eigenvectors are $\ket{\lambda_\pm} = \frac{1}{\sqrt{4 \mp 2 \sqrt{2}}} \begin{bmatrix}
1 \\ 
-1 \pm \sqrt{2}
\end{bmatrix} $.



\Textbf{2.54}
Since $[A, B] = 0$, $A$ and $B$ are simultaneously diagonalize, $A = \sum_i a_i \kb{i}$, $B = \sum_i b_i \kb{i}$.
\begin{align*}
	\exp (A) \exp (B) &= \left(\sum_i \exp (a_i) \kb{i}\right) \left(\sum_i \exp (b_i) \kb{i}\right) \\
		&= \sum_{i,j} \exp (a_i + b_j) \ket{i} \braket{i | j} \bra{j}\\
		&= \sum_{i,j} \exp (a_i + b_j) \kbt{i}{j} \delta_{i, j}\\
		&= \sum_i \exp (a_i +  b_i) \kb{i}\\
		&= \exp (A+B)
\end{align*}


\Textbf{2.55}
\begin{align*}
	H = \sum_E E \kb{E}
\end{align*}

\begin{align*}
	U(t_2 - t_1) U^\dagger (t_2 - t_1) &= \exp \left( - \frac{iH(t_2 - t_1)}{\hbar} \right)  \exp \left(  \frac{iH(t_2 - t_1)}{\hbar} \right)\\
		&= \sum_{E, E'} \left(\exp \left(- \frac{iE(t_2 - t_1)}{\hbar} \right) \kb{E} \right)
										\left(\exp \left(- \frac{iE'(t_2 - t_1)}{\hbar} \right) \kb{E'} \right)\\
		&= \sum_{E, E'} \left(\exp \left(- \frac{i(E-E')(t_2 - t_1)}{\hbar} \right) \kbt{E}{E'} \delta_{E,E'} \right) \\
		&= \sum_E \exp(0) \kb{E}\\
		&= \sum_E \kb{E}\\
		&= I
\end{align*}

Similarly, $U^\dagger (t_2 - t_1) U (t_2 - t_1) = I$.



\Textbf{2.56}

$U = \sum_i \lambda_i \kb{\lambda_i}$~~~ ($|\lambda_i| = 1$).
\begin{align*}
	\log (U) &= \sum_j \log (\lambda_j) \kb{\lambda_j} = \sum_j i \theta_j  \kb{\lambda_j} \text{ where } \theta_j = \arg (\lambda_j)\\
	K &= - i \log(U) = \sum_j \theta_j \kb{\lambda_j}.
\end{align*}

\begin{align*}
	K^\dagger = (-i \log U)^\dagger = \left(\sum_j \theta_j \kb{\lambda_j}\right)^\dagger
	= \sum_j \theta_j^* \kb{\lambda_j} = \sum_j \theta_j \kb{\lambda_j} = K
\end{align*}



\Textbf{2.57}
\begin{align*}
	&\ket{\phi} \equiv \frac{L_l \ket{\psi}}{\sqrt{\braket{\psi |L_l^\dagger L_l | \psi}}}\\
	&\braket{\phi | M_m^\dagger M_m | \phi} = \frac{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}}{\braket{\psi | L_l^\dagger L_l | \psi}} \\
%
	&\frac{M_m \ket{\phi}}{\sqrt{\braket{\phi | M_m^\dagger M_m | \phi}}} =
		\frac{M_m L_l \ket{\psi}}{\sqrt{\braket{\psi |L_l^\dagger L_l | \psi}}}
		\cdot
		\frac{ \sqrt{\braket{\psi | L_l^\dagger L_l | \psi}} }{ \sqrt{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}} }
		=
		\frac{M_m L_l \ket{\psi}}{ \sqrt{\braket{\psi | L_l^\dagger M_m^\dagger M_m L_l | \psi}} }
		=
		\frac{N_{lm} \ket{\psi}}{\sqrt{\braket{\psi |N^\dagger_{lm}  N_{lm}  | \psi}}}
\end{align*}



\Textbf{2.58}
\begin{align*}
	\braket{M} &= \braket{ \psi | M | \psi} = \braket{\psi | m | \psi} = m \braket{\psi | \psi} = m\\
	\braket{M^2} &= \braket{ \psi | M^2 | \psi} = \braket{\psi | m^2 | \psi} = m^2 \braket{\psi | \psi} = m^2\\
	\text{deviation} &= \braket{M^2} - \braket{M}^2 = m^2 - m^2 = 0.
\end{align*}


\Textbf{2.59}
\begin{align*}
	\braket{X} &= \braket{0 | X | 0} = \braket{0 | 1} = 0\\
	\braket{X^2} &= \braket{0 | X^2 | 0} = \braket{0 | X | 1} =\braket{0 | 0} = 1\\
	\text{standard deviation} &= \sqrt{ \braket{X^2} - \braket{X}^2 } = 1
\end{align*}


\Textbf{2.60}
\begin{align*}
\vec{v} \cdot \vec{\sigma} &= \sum_{i=1}^3 v_i \sigma_i\\
&= v_1 \begin{bmatrix}
0 & 1 \\ 
1 & 0
\end{bmatrix}
+ v_2 \begin{bmatrix}
0 & -i \\ 
i & 0
\end{bmatrix}
+ v_3 \begin{bmatrix}
1 & 0 \\ 
0 & -1
\end{bmatrix} \\
&= \begin{bmatrix}
v_3 & v_1 - i v_2 \\ 
v_1 + iv_2 & -v_3
\end{bmatrix}
\end{align*}

\begin{align*}
\det (\vec{v} \cdot \vec{\sigma}  - \lambda I) &= (v_3 - \lambda) (-v_3 - \lambda) - (v_1 - iv_2) (v_1 + iv_2)\\
&= \lambda^2 - (v_1^2 + v_2^2  + v_3^2)\\
&= \lambda^2 - 1 ~~~ (\because |\vec{v}| = 1)
\end{align*}
Eigenvalues are $\lambda = \pm 1$.


(i) if $\lambda = 1$
\begin{align*}
	\vec{v} \cdot \vec{\sigma}  - \lambda I &= \vec{v} \cdot \vec{\sigma}  - I\\	
		&= \begin{bmatrix}
		v_3 - 1 & v_1 - i v_2 \\ 
		v_1 + i v_2 & - v_3 - 1
		\end{bmatrix}
\end{align*}

Eigenvector is $\ket{\lambda_1} = \sqrt{ \frac{1+v_3}{2 }} \begin{bmatrix}
1 \\ 
\frac{1-v_3}{v_1 - iv_2}
\end{bmatrix} $.

\begin{align*}
	\kb{\lambda_1} &= \frac{1+v_3}{2 } \begin{bmatrix}
		1 \\ 
		\frac{1-v_3}{v_1 - iv_2}
		\end{bmatrix}
		\begin{bmatrix}
		1 &
		\frac{1-v_3}{v_1 + iv_2}
		\end{bmatrix}\\
%
	&=
	 \frac{1+v_3}{2 } \begin{bmatrix}
	 1 & \frac{v_1 - iv_2}{1 + v_3} \\ 
	 \frac{v_1 + iv_2}{1 + v_3} & \frac{1-v_3}{1+v_3}
	 \end{bmatrix} \\
	 &=
	 \frac{1}{2} \begin{bmatrix}
	 1+v_3 & v_1 - iv_2 \\ 
	 v_1 + iv_2 & 1 - v_3
	 \end{bmatrix} \\
	 &=
	  \frac{1}{2} \left( I + \begin{bmatrix}
	 v_3 & v_1 - iv_2 \\ 
	 v_1 + iv_2 & - v_3
	 \end{bmatrix} \right) \\
	 &=
	 \frac{1}{2} (I + \vec{v} \cdot \vec{\sigma} )
\end{align*}



(ii) If $\lambda = -1$.
\begin{align*}
	\vec{v} \cdot \vec{\sigma}  - \lambda I &= \vec{v} \cdot \vec{\sigma}  + I\\	
	&= \begin{bmatrix}
		v_3 + 1 & v_1 - i v_2 \\ 
		v_1 + i v_2 & - v_3 + 1
	\end{bmatrix}
\end{align*}

Eigenvalue is $\ket{\lambda_{-1}} = \sqrt{ \frac{1-v_3}{2 }} \begin{bmatrix}
1 \\ 
- \frac{1+v_3}{v_1 - iv_2}
\end{bmatrix} $.


\begin{align*}
	\kb{\lambda_{-1}} &= \frac{1 - v_3}{2} \begin{bmatrix}
	1 \\ 
	- \frac{1+v_3}{v_1 - iv_2}
	\end{bmatrix}
	\begin{bmatrix}
		1 & - \frac{1+v_3}{v_1 + iv_2}
	\end{bmatrix}\\
	&=
	\frac{1 - v_3}{2} \begin{bmatrix}
		1 & - \frac{v_1 - iv_2}{1 - v_3} \\ 
		- \frac{v_1 + iv_2}{1 - v_3} & \frac{1+v_3}{1 - v_3}
	\end{bmatrix} \\
	&=
	\frac{1}{2} \begin{bmatrix}
		1 - v_3 & -(v_1 - iv_2) \\ 
		- (v_1 + iv_2) & 1 + v_3
	\end{bmatrix} \\
	&=
	\frac{1}{2} \left( I - \begin{bmatrix}
		v_3 & v_1 - iv_2 \\ 
		(v_1 + iv_2 & - v_3
	\end{bmatrix} \right)\\
	&= \frac{1}{2} (I - \vec{v} \cdot \vec{\sigma} ).
\end{align*}



\Textbf{2.61}
\begin{align*}
	\braket{\lambda_1 | 0} \braket{0 | \lambda_1} &= \braket{0 | \lambda_1} \braket{\lambda_1 | 0}\\
		&= \braket{0 | \frac{1}{2} ( I + \vec{v} \cdot \vec{\sigma} ) | 0}\\
		&= \frac{1}{2} (1 + v_3)
\end{align*}

Post-measurement state is
\begin{align*}
	\frac{\ket{\lambda_1} \braket{\lambda_1 | 0}}{ \sqrt{\braket{0 | \lambda_1} \braket{\lambda_1 | 0}} } &= \frac{1}{\sqrt{\frac{1}{2} (1 + v_3)}}
	\cdot \frac{1}{2}
	\begin{bmatrix}
		1 + v_3 \\ 
		v_1 + iv_2
	\end{bmatrix} \\
		&= \sqrt{ \frac{1}{2}  (1 + v_3) } \begin{bmatrix}
		1 \\ 
		\frac{v_1 + iv_2}{1+v_3}
		\end{bmatrix} \\
		&=  \sqrt{ \frac{1 + v_3}{2} } \begin{bmatrix}
		1 \\ 
		\frac{1 - v_3}{v_1 - iv_2}
		\end{bmatrix} \\
		&= \ket{\lambda_1}.
\end{align*}



\Textbf{2.62}

Suppose $M_m$ is an measurement operator.
From the assumption, $E_m = M_m^\dagger M_m = M_m$.

Then
\begin{align*}
    \braket{\psi | E_m | \psi} = \braket{\psi | M_m | \psi} \geq 0.
\end{align*}
for all $\ket{\psi}$.

Since $M_m$ is positive operator, $M_m$ is Hermitian.
Therefore,
\begin{align*}
    E_m = M_m^\dagger M_m = M_m M_m = M_m^2 = M_m.
\end{align*}

Thus the measurement is a projective measurement.



\Textbf{2.63}
\begin{align*}
    M_m^\dagger M_m &= \sqrt{E_m} U_m^\dagger U_m \sqrt{E_m}\\
        &= \sqrt{E_m} I \sqrt{E_m}\\
        &= E_m.
\end{align*}

Since $E_m$ is POVM,  for arbitrary  unitary $U$, $M_m^\dagger M_m$ is POVM.



\Textbf{2.64}

Define $E_i = \kb{\psi_i}$ for $1 \leq i \leq m$ and $E_{m+1} = I - \sum_{i=1}^{m} E_i$. Then $\sum_{i=1}^{m+1} E_i = I$.
And $\braket{\psi_i | E_i | \psi_i} = \braket{\psi_i | \psi_i} \braket{\psi_i | \psi_i} = 1$.



\Textbf{2.65}
\begin{align*}
    \ket{+} \equiv \frac{\ket{0} + \ket{1}}{\sqrt{2}}, ~~~ \ket{-} \equiv \frac{\ket{0} - \ket{1}}{\sqrt{2}}
\end{align*}


\Textbf{2.66}
\begin{align*}
     X_1 Z_2 \left(\frac{\ket{00} + \ket{11}}{\sqrt{2}} \right) = \frac{\ket{10} - \ket{01}}{\sqrt{2}}
\end{align*}


\begin{align*}
     \braket{X_1 Z_2} = \left(\frac{\bra{00} + \bra{11}}{\sqrt{2}} \right) X_1 Z_2 \left(\frac{\ket{00} + \ket{11}}{\sqrt{2}} \right)
    = \frac{\bra{00} + \bra{11}}{\sqrt{2}}  \cdot \frac{\ket{10} - \ket{01}}{\sqrt{2}}
    = 0
\end{align*}



\Textbf{2.67}

Unsolved

$W \subset V \rightarrow V = W \oplus W^\perp$.

$U: W \rightarrow V$, $U': V \rightarrow V$.

$U' \ket{w} = U\ket{w}$

$U' \in \mathcal{L}(V)$

$U \in \mathcal{L}(W)$

$U' = U \oplus I$ ???


\Textbf{2.68}

$\ket{\psi} = \frac{\ket{00} + \ket{11}}{\sqrt{2}}$.

Suppose $\ket{a} = a_0 \ket{0}  + a_1\ket{1}$ and $\ket{b} = b_0 \ket{0}  + b_1\ket{1}$.
%
\begin{align*}
    \ket{a} \ket{b} = a_0 b_0 \ket{00} + a_0 b_1 \ket{01} + a_1 b_0 \ket{10} + a_1 b_1 \ket{11}.
\end{align*}

If $\ket{\psi} = \ket{a} \ket{b}$, then $a_0 b_0 = 1,~ a_0 b_1=0,~ a_1 b_0 = 0,~ a_1 b_1 = 1$ since $\{\ket{ij}\}$ is an orthonormal basis.

If $a_0 b_1 = 0$, then $a_0 = 0$ or $b_1 = 0$.

When $a_0 = 0$ , this is contradiction to $a_0 b_0 = 1$.
When $b_1 = 0$ , this is contradiction to $a_1 b_1 = 1$.

Thus $\ket{\psi} \neq \ket{a} \ket{b}$.


\Textbf{2.69}
Define Bell states as follows.
\begin{align*}
    \ket{\psi_1} &\equiv \frac{\ket{00} + \ket{11}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\ 
    0 \\ 
    0 \\ 
    1
    \end{bmatrix} \\
    \ket{\psi_2} &\equiv \frac{\ket{00} - \ket{11}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\ 
    0 \\ 
    0 \\ 
    -1
    \end{bmatrix} \\
    \ket{\psi_3} &\equiv \frac{\ket{01} + \ket{10}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    0 \\ 
    1 \\ 
    1 \\ 
    0
    \end{bmatrix} \\
    \ket{\psi_4} &\equiv \frac{\ket{01} - \ket{10}}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix}
    0 \\ 
    1 \\ 
    -1 \\ 
    0
    \end{bmatrix} \\
\end{align*}

First, we prove $\{\ket{\psi_i} \}$ is a linearly independent basis.
\begin{align*}
    &a_1 \ket{\psi_1} + a_2 \ket{\psi_2} + a_3 \ket{\psi_3} + a_4 \ket{\psi_4} = 0\\
    &\therefore \frac{1}{\sqrt{2}} \begin{bmatrix}
    a_1 + a_2 \\ 
    a_3 + a_4 \\ 
    a_3 - a_4 \\ 
    a_1 - a_2
    \end{bmatrix} = 0
\end{align*}
\begin{subnumcases}
 \therefore {}
a_1 + a_2 = 0& \nonumber \\
a_3+ a_4 = 0& \nonumber \\
a_3 - a_4 = 0& \nonumber \\
a_1 - a_2 = 0& \nonumber
\end{subnumcases}
\begin{align*}
    \therefore a_1 = a_2 = a_3 = a_4 = 0
\end{align*}
Thus $\{\ket{\psi_i}\}$ is a linearly independent basis.

Moreover $\norm{\ket{\psi_i}} = 1$ and $\braket{\psi_i | \psi_j} = \delta_{ij}$ for $i,j = 1, 2, 3, 4$.
Therefore $\{\ket{\psi_i}\}$ forms an orthonormal basis.


\Textbf{2.70}
For any Bell states we get $\braket{\psi_i | E \otimes I | \psi_i} = \frac{1}{2} (\braket{0|E|0} + \braket{1|E|1})$.

Suppose Eve measures the qubit Alice sent by measurement operators $M_m$.
The probability that Eve gets result $m$ is $p_i(m) = \braket{\psi_i | M_m^\dagger M_m \otimes I | \psi_i}$.
Since $M ^\dagger_m M_m$ is positive, $p_i(m)$ are same values for all $\ket{\psi_i}$.
Thus Eve can't distinguish Bell states.


\Textbf{2.71}
From spectral decomposition,
\begin{align*}
    \rho &= \sum_i p_i \kb{\psi_i}, ~~ p_i \geq 0, ~~ \sum_i p_i = 1.\\
    \rho^2 &= \sum_{i,j} p_i p_j \ket{i}\braket{i|j}\bra{j}\\
        &= \sum_{i,j} p_i p_j \kbt{i}{j}\delta_{ij}\\
        &= \sum_i p_i^2 \kb{i}
\end{align*}

\begin{align*}
    \Tr (\rho^2) &= \Tr \left(\sum_i p_i^2 \kb{i}\right)
        = \sum_i p_i^2 \Tr(\kb{i})
        = \sum_i p_i^2 \braket{i|i}
        = \sum_i p_i^2
        \leq \sum_i p_i = 1~~~ (\because p_i^2 \leq p_i)
\end{align*}

Suppose $\Tr (\rho^2) = 1$. Then $\sum_i p_i^2 = 1$.
If $0 \leq p_i < 1$, then $p_i^2 < p_i$.
Thus only one $p_i = 1$ and otherwise are $0$.
Therefore $\rho = \kb{\psi_i}$ is pure state.

Conversely if $\rho$ is pure, then $\rho = \kb{\psi}$.
\begin{align*}
    \Tr (\rho^2) = \Tr (\ket{\psi}\braket{\psi | \psi} \bra{\psi}) = \Tr (\kb{\psi}) = \braket{\psi | \psi} = 1.
\end{align*}



\Textbf{2.72}
(1)

Since density matrix is Hermitian, matrix representation is
$\rho = \begin{bmatrix}
a & b \\ b^* & d
\end{bmatrix}$,
$a, d \in \mathds{R}$ and $b \in \mathds{C}$ w.r.t. standard basis.
Because $\rho$ is density matrix, $\Tr (\rho) = a+d = 1$.

Define $a = (1+r_3)/2$, $d = (1-r_3)/2$ and $b = (r_1 - ir_2)/2$, $(r_i \in \mathds{R})$.

In this case,
\begin{align*}
    \rho = \begin{bmatrix}
        a & b \\ b^* & d
    \end{bmatrix}
    =
    \frac{1}{2} \begin{bmatrix}
        1+r_3 & r_1 - ir_2 \\ 
        r_1 + ir_2 & 1 - r_3
    \end{bmatrix}
    =
    \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma}).
\end{align*}
Thus for arbitrary density matrix $\rho$ can be written as $\rho = \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})$.

Next, we derive the condition that $\rho$ is positive.

If $\rho$ is positive, all eigenvalues of $\rho$ should be non-negative.
\begin{align*}
    \det (\rho - \lambda I) &= (a-  \lambda) (b - \lambda) - |b|^2 = \lambda^2 - (a+d)\lambda + ad - |b^2| = 0\\
    \lambda &= \frac{(a+d) \pm \sqrt{(a+d)^2 - 4 (ad - |b|^2)}}{2}\\
        &= \frac{1 \pm \sqrt{1 - 4 \left(\frac{1 - r_3^2}{4} - \frac{r_1^2 + r_2^2}{4} \right)}}{2}\\
        &= \frac{1 \pm \sqrt{1 - (1 - r_1^2 - r_2^2 - r_3^2)}}{2}\\
        &= \frac{1 \pm \sqrt{|\vec{r}|^2}}{2}\\
        &= \frac{1 \pm |\vec{r}|}{2}
\end{align*}

Since $\rho$ is positive, $\frac{1 - |\vec{r}|}{2} \geq 0 \rightarrow |\vec{r}| \leq 1$.

Therefore an arbitrary density matrix for a mixed state qubit is written as $\rho = \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})$.

\vspace{5mm}
(2)

$\rho = I / 2 \rightarrow \vec{r}  = 0$. Thus  $\rho = I / 2$ corresponds to the origin of Bloch sphere.

\vspace{5mm}
(3)

\begin{align*}
    \rho^2 &= \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})~ \frac{1}{2} (I + \vec{r} \cdot \vec{\sigma})\\
        &= \frac{1}{4} \left[ I + 2 \vec{r}\cdot \vec{\sigma} + \sum_{j,k}r_j r_k \left(\delta_{jk} I + i \sum_{l=1}^3 \epsilon_{jkl}\sigma_l \right)  \right]\\
        &= \frac{1}{4} \left(I + 2 \vec{r}\cdot \vec{\sigma} + |\vec{r}|^2 I \right)\\
    \Tr (\rho^2) &= \frac{1}{4} (2 + 2|\vec{r}|^2)
\end{align*}

If $\rho$ is pure, then $\Tr (\rho^2) = 1$.
\begin{align*}
   1 =  \Tr (\rho^2) = \frac{1}{4} (2 + 2|\vec{r}|^2)\\
   \therefore |\vec{r}| = 1.
\end{align*}

Conversely, if $|\vec{r}| = 1$, then $\Tr (\rho^2) = \frac{1}{4} (2 + 2|\vec{r}|^2) = 1$. Therefore $\rho$ is pure.




\Textbf{2.73}
\begin{screen}
    \Textbf{Theorem 2.6}
%
    \begin{align*}
        \rho = \sum_i p_i \kb{\psi_i} = \sum_i \kb{\tilde{\psi_i}} = \sum_j \kb{\tilde{\varphi}_j} = \sum_j q_j \kb{\varphi_j} ~~ \Leftrightarrow ~~ \ket{\tilde{\psi}_i} = \sum_j u_{ij} \ket{\tilde{\varphi}_j}
    \end{align*}
    where $u$ is unitary.

	Transformation in theorem 2.6, $\ket{\tilde{\psi}_i} = \sum_j u_{ij} \ket{\tilde{\varphi}_j}$, corresponds to
	\begin{align*}
	    \left[ \ket{\tilde{\psi}_1} \cdots \ket{\tilde{\psi}_k} \right] = \Big[ \ket{\tilde{\varphi}_1} \cdots \ket{\tilde{\varphi}_k} \Big] U^T
	\end{align*}
	where $k = \mathrm{rank} (\mathcal{\rho})$.
\end{screen}

From spectral theorem, density matrix $\rho$ is decomposed as $\rho = \sum_{k=1}^{d} \lambda_k \kb{k}$ where $d = \dim \mathcal{H}$.
Without loss of generality, we can assume $p_k > 0$ for $k = 1 \cdots , l$ where $l = \mathrm{rank} (\rho)$ and $p_k = 0$ for $k = l+1, \cdots, d$.
Thus $\rho = \sum_{k=1}^{l} p_k \kb{k} = \sum_{k=1}^{l} \kb{\tilde{k}}$, where $\ket{\tilde{k}} = \sqrt{\lambda_k} \ket{k}$.

Suppose $\ket{\psi_i}$ is a state in support $\rho$. Then
\begin{align*}
	\ket{\psi_i} = \sum_{k=1}^l c_{ik} \ket{k}, ~~ \sum_k |c_{ik}|^2 = 1.
\end{align*}

Define $\displaystyle p_i = \frac{1}{\sum_k \frac{|c_{ik}|^2}{\lambda_k} }$ and $\displaystyle u_{ik} = \frac{\sqrt{p_i} c_{ik}}{\sqrt{\lambda_k}}$.

Now
\begin{align*}
	\sum_k |u_{ik}|^2 = \sum_k \frac{p_i | c_{ik} |^2 }{\lambda_k} = p_i \sum_k \frac{| c_{ik} |^2 }{\lambda_k} = 1.
\end{align*}

Next prepare an unitary operator such that $(i, k)$ component of $U$ is $u_{ik}$.
Then we can define another ensemble such that
\begin{align*}
	\Big[  \ket{\tilde{\psi}_1} \cdots  \ket{\tilde{\psi}_i} \cdots \ket{\tilde{\psi}_l}\Big] = \Big[ \ket{\tilde{k}_1} \cdots \ket{\tilde{k}_l} \Big] U^T
\end{align*}
where $\ket{\tilde{\psi_i}} = \sqrt{p_i} \ket{\psi_i}$.
From theorem 2.6,
\begin{align*}
	\rho = \sum_k \kb{\tilde{k}} = \sum_k \kb{\tilde{\psi}_k}.
\end{align*}

Therefore we can obtain a minimal ensemble for $\rho$ that contains $\ket{\psi_i}$.

Moreover since $\rho^{-1} = \sum_k \frac{1}{\lambda_k} \kb{k}$,
\begin{align*}
	\braket{\psi_i | \rho^{-1} | \psi_i} = \sum_k \frac{1}{\lambda_k} \braket{\psi_i | k} \hspace{-1mm} \braket{k | \psi_i} = \sum_k \frac{|c_{ik}|^2}{\lambda_k} = \frac{1}{p_i}.
\end{align*}

Hence, $ \frac{1}{\braket{\psi_i | \rho^{-1} | \psi_i}} = p_i $.


\Textbf{2.74}




\Textbf{2.75}
\Textbf{2.76}
\Textbf{2.77}
\Textbf{2.78}
\Textbf{2.79}
\Textbf{2.80}
\Textbf{2.81}
\Textbf{2.82}

